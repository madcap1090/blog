---
title: Metallica rules the world
author: William Bourgeois
date: '2019-05-24'
slug: metallica-rules-the-world
categories: []
tags:
  - Geocoding
  - music
  - scraping
  - rbokeh
---

23 days till Metallica hits the Koning Boudewijnstadion in Belgium. High time to get loaded with some Metallica data. There is an interesting site called [setlist](https://www.setlist.fm) that publishes setlists of loads of bands and indeed also of Metallica. It also lists the name of the tour, the venues and the dates. Pretty cool stuff that can be scraped and analysed. And it even says it cares about our privacy, perfect! 

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/FtUptw2ZmDM" frameborder="0" allowfullscreen></iframe>
</center>


```{r message=FALSE, warning=FALSE}
library("rbokeh") # install.packages("rbokeh")
library("tidyverse")
library("maps")
library("lubridate")
library("rvest")
library("httr")
library("glue")
```


In order to scrape the pages we want, a tibble of urls can be constructed in this manner:

```{r}
url <- "https://www.setlist.fm/setlists/metallica-3bd680c8.html?page=1"
```

This is the first page showing the latests concerts of Metallica. On that page there is one element that can be used, the total number of Metallica pages on the website (today a whopping 202!). 

We read the page and get that element.

```{r}
selector <- "body > div.body > div.container > div.row.main > div.mainColumn.col-xs-12.col-md-8 > div:nth-child(2) > div > div > div.col-xs-12.noTopBorder.noTopPadding.hidden-print.text-center.listPager-lg > ul > li:nth-child(9) > a"
page <- read_html(url)

n_pages <- page %>% 
  html_node(selector) %>% 
  html_text
n_pages #number of pages

```


So our object will have 202 rows. It's also a nifty trick using the function glue. 

```{r}

pages <- c(1:n_pages) # the vector of the pages
urls <-  glue("https://www.setlist.fm/setlists/metallica-3bd680c8.html?page={pages}") %>% 
  enframe(name = NULL)
head(urls)

```

We also might want to use this code in the future not to scrape the 202 pages again, but just the latest concerts. So going back to the future we can look for a file where we stored our results from the latest scraping and extract the latest date. 

```{r}
if(file.exists("./data/20190524/scraping.rds")){
  max_date <- readRDS("./data/20190524/scraping.rds") %>% 
    select("date") %>% 
    filter(date == max(date)) %>%
    unique() %>% 
    '[['(1) # funky ;-)
    } else{
  max_date <- ymd("1966-01-01")
  }
max_date
```

So now we can continue to prepare the scraping by getting the pages we need to download.

```{r}

# selector
#selector_url_concert <- ".url"

# function
get_urls <- function(x){
  read_html(x) %>% 
  html_nodes(".url")
  }
concert_urls <- tibble() # empty tibble to be filled

for(i in urls$value){
    # evaluate if we already have the data
  selector <- "body > div.body > div.container > div.row.main > div.mainColumn.col-xs-12.col-md-8 > div:nth-child(2) > div > div > div:nth-child(1) > div:nth-child(1) > div"
  date <- read_html(i) %>% 
    html_node(selector) %>% 
    html_text
  date <- str_replace_all(date, "\\\n", " ") %>% 
    mdy()
  
  if (date < max_date) break # break here if we have it, continue if we don't
  
  print(i)
  new_url_nodes <- get_urls(i) 
  print(new_url_nodes)
  
  urls <- tibble() # empty tibble to be filled
  
  for (j in (1:length(new_url_nodes))) {
    url <- xml_attrs(new_url_nodes[[j]])[["href"]] %>% 
    as_tibble()  
    urls <- rbind(urls, url)
  }
  concert_urls <- rbind(concert_urls, urls)
}

```

At this point 'concert_url' contains the urls of the concert we did not have in the data since our latest future scraping. We need to suffix it with "https://www.setlist.fm". 

```{r}
concert_urls <- concert_urls %>% 
  mutate(url = str_replace(value,"..","https://www.setlist.fm")) %>% 
  select(-value)
```

Getting there. All systems are go.

```{r}
concert_urls <- add_column(concert_urls, date = "", tour ="", venue ="",
                           setlist ="") # create space to add to the df
```


```{r}
n <- nrow(concert_urls)

for(i in (1:n)) {
  
  # download page
  url <- concert_urls$url[i]
  page <- read_html(url)

  # get tour
  tour <- page %>%
    html_node("p > span:nth-child(2)") %>%
    html_text()
  
  concert_urls$tour[i] <- tour

  # get date
  year <- page %>%
    html_node(".year") %>%
    html_text
  
  month <- page %>%
    html_node(".month") %>%
    html_text
  
  day <- page %>%
    html_node(".day") %>%
    html_text
  
  date <- mdy(paste(month, day, year))

  print(date)

 concert_urls$date[i] <- date

  # get venue
  venue <- page %>%
    html_nodes("div.infoContainer > div > h1 > span > span > a > span") %>%
    html_text()
  
  concert_urls$venue[i] <- venue
  
  # get setlist
  setlist <- page %>%
      html_nodes(".songLabel") %>%
      html_text()
    
  concert_urls$setlist[i] <- list(setlist)
  
  Sys.sleep(1) # do not stress the website, we have time
}
```

Ok, we can now do some data cleaning. And add the new data to the older data if we are not scaping in this way for the first time. 


```{r}
concert_urls$date <- as.numeric(concert_urls$date)
concert_urls$date <- as.Date(concert_urls$date, origin = "1969-12-30")
concert_urls_new <- concert_urls

if(file.exists("./data/20190524/scraping.rds")){
  concert_urls_old <- readRDS("./data/20190524/scraping.rds") %>% 
    filter(!date %in% concert_urls_new$date)
    } else{
  concert_urls_old <- data.frame()
  }

concert_urls <- rbind(concert_urls_new,concert_urls_old)
```

Happy me I am. We tanked data on the 2K concerts of Metallica. And we know some more are coming. :metal: 

```{r}
saveRDS(concert_urls, "./data/20190524/scraping.rds")

```

