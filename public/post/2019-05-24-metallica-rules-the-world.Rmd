---
title: Metallica rules the world
author: William Bourgeois
date: '2019-05-24'
slug: metallica-rules-the-world
categories: []
tags:
  - Geocoding
  - music
  - scraping
  - rbokeh
  - opencage
  - Metallica
---

23 days till Metallica hits the Koning Boudewijnstadion in Belgium. High time to get loaded with some Metallica data. There is an interesting site called [setlist](https://www.setlist.fm) that publishes setlists of loads of bands and indeed also of Metallica. It also lists the name of the tour, the venues and the dates. Pretty cool stuff that can be scraped and analysed. 

And the site even says it cares about our privacy, that is just perfect! 

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/FtUptw2ZmDM" frameborder="0" allowfullscreen></iframe>
</center>



```{r message=FALSE, warning=FALSE}
library("rbokeh") # install.packages("rbokeh")
library("tidyverse")
library("maps")
library("lubridate")
library("rvest")
library("httr")
library("glue")
library("measurements")  # install.packages("measurements")
library("stringr")
library("ggthemes")
library("spotifyr") # install.packages("spotifyr")
```


In order to scrape the pages we want, a tibble of urls can be constructed in this manner:

```{r}
url <- "https://www.setlist.fm/setlists/metallica-3bd680c8.html?page=1"
```

This is the first page showing the latest concerts of Metallica. On that page there is one element that can be used, the total number of Metallica pages on the website (today a whopping 202!). 

We read the page and get that element.

```{r}
selector <- "body > div.body > div.container > div.row.main > div.mainColumn.col-xs-12.col-md-8 > div:nth-child(2) > div > div > div.col-xs-12.noTopBorder.noTopPadding.hidden-print.text-center.listPager-lg > ul > li:nth-child(9) > a"
page <- read_html(url)

n_pages <- page %>% 
  html_node(selector) %>% 
  html_text
n_pages #number of pages

```


So our object will have 202 rows. It's also a nifty trick using the function glue. 

```{r}

pages <- c(1:n_pages) # the vector of the pages
urls <-  glue("https://www.setlist.fm/setlists/metallica-3bd680c8.html?page={pages}") %>% 
  enframe(name = NULL)
head(urls)

```

We also might want to use this code in the future not to scrape the 202 pages again, but just the latest concerts. So going back to the future we can look for a file where we stored our results from the latest scraping and extract the latest date. 

```{r}
if(file.exists("./data/20190524/scraping.rds")){
  max_date <- readRDS("./data/20190524/scraping.rds") %>% 
    select("date") %>% 
    filter(date == max(date)) %>%
    unique() %>% 
    '[['(1) # funky ;-)
    } else{
  max_date <- ymd("1966-01-01")
  }
max_date
```

So now we can continue to prepare the scraping by getting the pages we need to download.

```{r warning=FALSE}

# selector
#selector_url_concert <- ".url"

# function
get_urls <- function(x){
  read_html(x) %>% 
  html_nodes(".url")
  }
concert_urls <- tibble() # empty tibble to be filled

for(i in urls$value){
    # evaluate if we already have the data
  selector <- "body > div.body > div.container > div.row.main > div.mainColumn.col-xs-12.col-md-8 > div:nth-child(2) > div > div > div:nth-child(1) > div:nth-child(1) > div"
  date <- read_html(i) %>% 
    html_node(selector) %>% 
    html_text
  date <- str_replace_all(date, "\\\n", " ") %>% 
    mdy()
  
  if (date < max_date) break # break here if we have it, continue if we don't
  
  print(i)
  new_url_nodes <- get_urls(i) 
  print(new_url_nodes)
  
  urls <- tibble() # empty tibble to be filled
  
  for (j in (1:length(new_url_nodes))) {
    url <- xml_attrs(new_url_nodes[[j]])[["href"]] %>% 
    as_tibble()  
    urls <- rbind(urls, url)
  }
  concert_urls <- rbind(concert_urls, urls)
}

```

At this point 'concert_url' contains the urls of the concert we did not have in the data since our latest future scraping. We need to suffix it with "https://www.setlist.fm". 

```{r}
concert_urls <- concert_urls %>% 
  mutate(url = str_replace(value,"..","https://www.setlist.fm")) %>% 
  select(-value)
```

Getting there. All systems are go.

```{r}
concert_urls <- add_column(concert_urls, date = "", tour ="", venue ="",
                           setlist ="") # create space to add to the df
```


```{r}
n <- nrow(concert_urls)

for(i in (1:n)) {
  
  # download page
  url <- concert_urls$url[i]
  page <- read_html(url)

  # get tour
  tour <- page %>%
    html_node("p > span:nth-child(2)") %>%
    html_text()
  
  concert_urls$tour[i] <- tour

  # get date
  year <- page %>%
    html_node(".year") %>%
    html_text
  
  month <- page %>%
    html_node(".month") %>%
    html_text
  
  day <- page %>%
    html_node(".day") %>%
    html_text
  
  date <- mdy(paste(month, day, year))

  print(date)

 concert_urls$date[i] <- date

  # get venue
  venue <- page %>%
    html_nodes("div.infoContainer > div > h1 > span > span > a > span") %>%
    html_text()
  
  concert_urls$venue[i] <- venue
  
  # get setlist
  setlist <- page %>%
      html_nodes(".songLabel") %>%
      html_text()
    
  concert_urls$setlist[i] <- list(setlist)
  
  Sys.sleep(1) # do not stress the website, we have time
}
```

Ok, we can now do some data cleaning. And add the new data to the older data if we are not scraping in this way for the first time. 


```{r}
concert_urls$date <- as.numeric(concert_urls$date)
concert_urls$date <- as.Date(concert_urls$date, origin = "1969-12-30")
concert_urls_new <- concert_urls

if(file.exists("./data/20190524/scraping.rds")){
  concert_urls_old <- readRDS("./data/20190524/scraping.rds") %>% 
    filter(!date %in% concert_urls_new$date)
    } else{
  concert_urls_old <- data.frame()
  }

concerts <- rbind(concert_urls_new,concert_urls_old)
```

Happy me I am. We tanked data on the 2K concerts of Metallica. And we know some more are coming.
Saving them for later.

```{r}
saveRDS(concerts, "./data/20190524/scraping.rds")
```

In the data frame there are also the locations of the concerts:

```{r}
set.seed(42)
concerts %>% 
  sample_n(10) %>% 
  select(venue)

```


This time we can use a different package and api to geocode the concert venues. I've been planning to test [opencage]("https://ropensci.org/tutorials/opencage_tutorial/") for a while.

It offers 2.500 calls or queries per day using the free trial plan. 

How many different venues do we have?

```{r}
concerts %>% 
  select(venue) %>% 
  unique %>% 
  nrow()
```

So we should have enough.

```{r}
library("opencage") # install.packages("opencage")
```

Get your api key and set it as an environment variable "OPENCAGE_KEY"

```{r}
key <- readRDS("~/R/geo/secret/secrets.rds") %>% 
  filter(name == "opencage") %>% 
  select(key) %>% 
  as.character()

Sys.setenv(OPENCAGE_KEY = key)

# check with
#Sys.getenv("OPENCAGE_KEY")

```


A test on one venue:

```{r}
output <- opencage_forward(placename = "Göta Lejon, Stockholm, Sweden")
output$rate_info
```

So one less request remaining.

What interested me about opencage was that it gives us a whole range of information (73 variables). The output comes in lists and a data frame:

```{r}
str(output)
```

Although all that information might not be relevant for this blog subject. Except maybe for:

```{r}
output[["results"]][["components._type"]]
```

A data frame can be used containing the unique venues to loop through or map and add the lat, lon and maybe 'components._type' and 'formatted' which seems to be the estimation of the correct address.

```{r}
output[["results"]][["formatted"]][1]
```

The data frame:

```{r}
venues <- concerts %>% 
  select(venue) %>% 
  unique
```

The loop:

```{r}
test_venues <- venues %>% 
  slice(1:5) 

# adding columns to fill them

test_venues <- add_column(test_venues, lat = "", lon = ""
                          , type = "", address = "")

for (i in (1:5)){
  print(test_venues$venue[i])
  place <- test_venues$venue[i]
  output <- opencage_forward(placename = place)
  test_venues$lat[i] <- as.character(output[["results"]][["annotations.DMS.lat"]][1])
  test_venues$lon[i] <- as.character(output[["results"]][["annotations.DMS.lng"]][1])
  test_venues$type[i] <-as.character(output[["results"]][["components._type"]][1])
  test_venues$address[i] <- as.character(output[["results"]][["formatted"]][1])
}




```

Seems to work, 'do' them all. But an error was returned at "Carlini Dome, King George Island, Antarctica". So a 'next' is needed in the loop.

```{r eval=FALSE}

venues <- add_column(venues, lat = "", lon = ""
                          , type = "", address = "")

n <- nrow(venues)

for (i in (1:n)){
  print(venues$venue[i])
  place <- venues$venue[i]
  output <- opencage_forward(placename = place)
  if (length(as.character(output[["results"]][["annotations.DMS.lat"]][1])) == 0) next
  venues$lat[i] <- as.character(output[["results"]][["annotations.DMS.lat"]][1])
  venues$lon[i] <- as.character(output[["results"]][["annotations.DMS.lng"]][1])
  venues$type[i] <-as.character(output[["results"]][["components._type"]][1])
  venues$address[i] <- as.character(output[["results"]][["formatted"]][1])
}


```

Since we are limited and it took a while, we can save the results for later.

```{r}
# saveRDS(venues, "./data/20190524/venues.rds")
```

```{r}
venues <-  readRDS("./data/20190524/venues.rds")
```


Either they got real creative or the variable 'type' is not very useful:

```{r}
unique(venues$type)
```


```{r}
venues %>% 
   group_by(type) %>% 
   summarise(count = n()) %>% 
  arrange(desc(count))

```

Converting lat and lon (gives a warning because of the venue in Antarctica, so we slice it away).


```{r}

venues_coord <- venues %>% 
  filter(venue != "Carlini Dome, King George Island, Antarctica") %>% 
  mutate(NS = str_extract(lat, "[[:alpha:]]"),
         EW = str_extract(lon, "[[:alpha:]]"),
         lat = str_remove(lat,'°'),
         lon = str_remove(lon,'°'),
         lat = str_remove(lat,"[[:alpha:]]"),
         lon = str_remove(lon,"[[:alpha:]]"),
         lat = str_replace(lat,"''",' '),
         lon = str_replace(lon,"''",' '),
         lat = str_replace(lat,"'",' '),
         lon = str_replace(lon,"'",' '),
         lat = str_squish(lat),
         lon = str_squish(lon),
         lat = as.numeric(conv_unit(lat, from = "deg_min_sec", to = "dec_deg")),
         lon = as.numeric(conv_unit(lon, from = "deg_min_sec", to = "dec_deg")),
         lat = if_else(NS == "S", lat*-1, lat),
         lon = if_else(EW == "W", lon*-1, lon),
         )

```


Adding Antarctica:

```{r}
antartica <- venues %>% 
  slice(178) %>%
  mutate(lat = -62.2381424,
         lon = -58.6688106,
         NS = "S",
         EW = "W")

venues_coord <- rbind(venues_coord, antartica)

```

Adding the venue data to the concerts data frame.

```{r}
concerts <- concerts %>% 
  left_join(venues_coord)

# add country 

concerts <- concerts %>% 
  mutate(country = word(venue,-1),
         year = year(date))
```

Now we can construct an interactive map to explore where Metallica has performed their 2028 concerts.


```{r message=FALSE, warning=FALSE}
figure(width = 800, height = 450, padding_factor = 0) %>%
  ly_map("world", regions = ".", col = "gray") %>%
  ly_points(lon, lat, data = concerts, size = 6,
            color = year ,glyph = 8,
    hover = c(venue, tour, country, date))
```

The darker the green the more recent the concert. They mainly play North America and Europe. But as we saw also in Antarctica and in Tuktoyaktuk in Canada.

Where have they played in Belgium?


```{r message=FALSE, warning=FALSE}


venues <- concerts$venue
figure(xlim = (2:7.7), ylim = (49.5:51.5), width = 800, height = 450, padding_factor = 0) %>%
  ly_map("world", regions = "belgium", col = "gray") %>%
  ly_points(lon, lat, data = concerts, size = 6,
            color = year(date) ,glyph = 8,
    hover = c(venue, tour, country, date))



```


```{r}
concerts %>% 
  filter(country == "Belgium") %>% 
  select(-url, -setlist, -lat, -lon, -type,
         -address, -NS, -EW, -country, -year) %>% 
  arrange(date)

```


So they played twice in Vorst, twice in Flanders Expo, three times in Kiewit and a bunch of times at Torhout/Wechter. But they started in Belgium by playing three times in 1984 in... Poperinge. Of all places.


<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/-myxmLacD5E" frameborder="0" allowfullscreen></iframe>
</center>

Well that video explains it all. Next month it will be the 24th time Metallica plays in Belgium and looking at the map of Belgium, it will be the first time in the Boudewijn Stadium. I wonder what there set list will look like. In the data frame there is a list in the values 'setlist'. These can be unlisted by the formula 'unnest' (so every row is a track played). And I keep the latest tour only.


```{r}
worldwired <- concerts %>% 
  filter(tour == "Worldwired Tour") %>% 
  unnest(setlist) 
```

How many songs per set?

```{r}
count <- worldwired %>% group_by(date) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count)) 
round(mean(count$count), 2)  
```

So that is 18 tracks per set give or take. But which ones will they play next month in Brussels? Most played songs:

```{r}
worldwired %>% group_by(setlist) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))
```

So if I sort per set date, start at the beginnig of 2017, arrange by the number of times the song got played and pour it into a graph it's easier to get an idea of what might be played.

```{r}
recent <- worldwired %>% 
   select(date, setlist) %>% 
   filter(year(date) > 2017)

count_after_2017 <- worldwired %>% 
  select(date, setlist) %>% 
  filter(year(date) > 2017) %>% 
  group_by(setlist) %>% 
  summarise(sum = n()) %>% 
  filter(sum > 0) 

 arranged <- recent %>% 
   left_join(count_after_2017)
 
 arranged <- arranged %>% 
  mutate(setlist = fct_reorder(setlist, sum))

```

```{r fig.height=13, fig.width=7}
ggplot(arranged, aes(as.factor(date), setlist)) +
  geom_point()+
  theme_economist()+scale_color_economist()+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```

The top nine tracks are a no brainer. There is a high chance they will get played. That leaves 9 other to guess. 'Creeping Death' and 'The Unforgiven' seem to have replaced 'Now That We're Dead' and 'Atlas, Rise!' as fixed items on the current set list. Looking at the latest concerts there seems to be a pattern emerging. If confirmed that means that the next good candidates would be: 'The Memory Remains', 'Here Comes Revenge', Ride the Lightning', 'Lords of Summer' and 'The God that Failed'. Outsiders would be: 'Frantic', 'Disposable Heroes', 'No  Leaf Cliver', 'St. Anger' and 'The Thing that Should Not Be'. 

But your geuss is as good as mine. 

I hope they will play 'Now That We're Dead'. Kind of like it. 


<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/QlF4rhAbwyc" frameborder="0" allowfullscreen></iframe>
</center>


See you at the concert!  :metal: 
