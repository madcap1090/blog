---
title: tf_idf and beer reviews
author: William Bourgeois
date: '2019-11-17'
slug: tf-idf-and-beer-reviews
categories: []
tags:
  - Beers
  - nlp
  - language detection
---

We have our data scraped from a beer website now, we scraped them from the web like we showed [here](https://badtothecode.netlify.com/post/beer-reviews-galore/). Where we best start looking at the data?

Maybe best by a tf-idf analysis? I like it because it is simple, but at the same time gives valuable insights both into the structure of the data (words by category) and the content (differences between categories of texts, whatever you might decide these categories to be)

For every word in every 'category' it calculates the term frequency against the inverse document frequency. More [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).


```{r message=FALSE, warning=FALSE}
library("tidyverse")
library("tidytext") # install.packages("tidytext")
library("cld2")
library("cld3") #install.packages("cld3")
library("DT")
library("ggrepel")
library("plotly")
library("viridis")
```


After attaching the libraries and loading the data, we can create a function that will clean the reviews and delete some words that directly link to the beer category and that therefore they do not add any information. 


```{r}

input <- readRDS("./data/20191113/output_final_cleaned.rds")

treat_comment <- function(text){
  text <- paste(unlist(text), collapse = "") 
  text <- gsub("characters", "characters ", text)
  text <- gsub('[[:digit:]]+',' ',text)
  text <- gsub('[[:punct:]]+',' ',text)
  text <- gsub('[^[:alpha:]]+',' ',text)
  text <- gsub('[^[:alpha:]]+',' ',text)
  text <- tolower(text)
  text <- gsub("belgian",' ', text)
  text <- gsub("lambic",' ', text)
  text <- gsub("quad",' ', text)
  text <- gsub("strong dark",' ', text)
  text <- gsub("strong pale",' ', text)
  text <- gsub("tripel",' ', text)
  text <- gsub("bsda",' ', text)
  text <- gsub("bsdas",' ', text)
  text <- gsub('rdev look smell taste feel overall'," ",text)
  return(text)
}

```

Let's see a review #15 as a sample off a treated review.

```{r}
input %>% select(i, comments) %>% 
  slice(15) %>% 
  mutate(comments = substr(treat_comment(comments),1,1500)) %>% 
  datatable(rownames = FALSE,  options = list(dom  = 't'))
```

The reviews are all still in one list per beer, so it is time to get them out of the list. In this blog post we also will only use the beer category.

```{r}
input <- input %>% 
  select(beer_type, comments) %>% 
  unnest(cols = c(beer_type, comments))
```

So we increased the number of rows of our data frame from 3.978 (the number of beers) to 154.896 (the number of reviews).

Next we apply our function using purrr's map_ch function. I also filter out small comments. This takes about a minute and a half.

```{r}
system.time(
input_text <- input %>%
  mutate(texts = map_chr(comments, treat_comment)) %>% 
  select(-comments) %>% 
  filter(str_length(texts) > 40)
)
```

So that should be one review per row.

```{r}
head(input_text) %>%
  select(texts)
```

Next up is language detection. cld3 is based on Google's Compact Language Detector v3, a neural network model for language identification. It follows cld2 that used a Na√Øve Bayesian classifier. Let's organise a little competition between the two. 

```{r}
system.time(
input_text_language_3 <- input_text %>%
  mutate(language_3 = cld3::detect_language(text = texts)) 
)
```

```{r}
system.time(
input_text_language_2 <- input_text %>%
  mutate(language_2 = cld2::detect_language(text = texts)) 
)

```

So indeed cl2 is about 50 times faster. What about the language detection? How often do they disagree?

```{r}
input_text_language <- input_text_language_2 %>% 
  cbind(input_text_language_3$language_3) %>% 
  rename("language_3" = "input_text_language_3$language_3") %>% 
  mutate(diff = (language_2 == language_3))

diff <- input_text_language %>% 
  filter(diff == FALSE)

nrow(diff)
```

So that is not a big difference at all. Here is an example of where they do not give the same result:

```{r}
diff %>% 
  select(language_2, texts) %>% 
  slice(16) %>%
  mutate(texts = substr(texts,1,1200)) %>% 
   datatable(rownames = FALSE,  options = list(dom  = 't'))
```


Both packages have a function to detect multiple languages, but cld2 seems better at it. At least on these 16 texts. 


```{r message=FALSE, warning=FALSE}
cld2::detect_language_mixed(diff$texts[16])
```

```{r}
cld3::detect_language_mixed(diff$texts[16])
```


Since we want the least possible foreign words we need to only keep the text classified as English texts.

What other languages do we have?

```{r message=FALSE, warning=FALSE}
input_text_language %>% 
  group_by(language_2, language_3) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))
```

And what type of beers are the most reviewed?

```{r}
input_text_language <- input_text_language %>% 
  filter(language_3 == language_2) %>% 
  filter(language_2 == "en") %>% 
  rename("language" = "language_2") %>% 
  select(beer_type, texts)

top <- 20

top_type <- input_text_language %>% 
  group_by(beer_type) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count)) %>% 
  slice(1:top)

top_type %>% head(15)

```

We'll only look at the top commented beers with the tf-idf, because otherwise the stop words become more 'special' in the categories with the most words. In other words we need to have enough words (and stop words) in each category. 

And we do not need to remove stop words because they will have low tf-idf scores. (We could even use the calculation to find stop-like words in a set for other types of analyses.)

Now we take all words from the text, count them and calculate the tf, idf and tf_idf scores.

```{r}
beer_type_word <- input_text_language %>%
  filter(beer_type %in% top_type$beer_type) %>% 
  unnest_tokens(word, texts) %>%
  count(beer_type, word, sort = TRUE) %>%
  ungroup() %>% 
  rename(count=n) %>%
  bind_tf_idf(word, beer_type, count)

beer_type_word %>%
  filter(beer_type =="Belgian Saison") %>% 
  select(-beer_type) %>% 
  arrange(desc(tf_idf))
```

So the word 'fantome' is, given its relative presence (2231 times) in the category and 

```{r}
beer_type_word %>% filter(word == "fantome") %>%
  select(count) %>% 
  sum() - 2231

```

its relative absence in the other categories (70 times), the most special word in the category compared 

```{r}
beer_type_word %>% filter(beer_type == "Belgian Saison") %>%
  select(count) %>% 
  sum()

```

to all the words in the category (1.355.455 words).

For the Belgian strong dark ales the words are:

```{r}
beer_type_word %>% 
  filter(beer_type == "Belgian Strong Dark Ale") %>%
  select(-beer_type) %>% 
  arrange(desc(tf_idf))
```

Let us try to visualise this information. Here 15 with top tf_idf, and the number of times they appeared in the category. 

```{r}
df <- beer_type_word %>% filter(beer_type =="Belgian Strong Dark Ale") %>% 
  arrange(desc(tf_idf)) %>% 
  slice(1:15)

ggplot(data = df, aes(x = count, y = tf_idf, label = word))+
  geom_point(size=.1, shape=0) + 
  geom_text_repel() +
  theme_minimal()
```


Here more words and using plotly, so one can zoom in the areas the words appear too closely.

```{r}
df <- beer_type_word %>% filter(beer_type =="Belgian Strong Dark Ale") %>%
  arrange(desc(tf_idf)) %>% 
  slice(1:35)

p <- ggplot(data = df, aes(x = count, y = tf_idf, label = word))+
  geom_point(size=.1, shape=0, color ="white") + 
  geom_text(label=df$word) +
  theme_minimal()

ggplotly(p, tooltip = c("tf_idf", "count"))

```

These are ok, but I guess I will need to learn Shiny soon.

Now for an overview of the top most rated beer types:

```{r message=FALSE, warning=FALSE, fig.width=12, fig.height=16, fig.align="center"}
top <- 6

top_type <- input_text_language %>% 
  group_by(beer_type) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count)) %>% 
  slice(1:top)

beer_type_word %>%
  filter(beer_type %in% top_type$beer_type) %>% 
  mutate(word = make.names(word, unique = TRUE)) %>% 
  arrange(beer_type, desc(-tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(beer_type) %>% 
  top_n(15) %>%
  ungroup() %>%
  ggplot(aes(x = reorder(word, tf_idf), y = tf_idf, fill = beer_type)) +
  scale_fill_viridis(discrete = TRUE, option = "D")+
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~beer_type, ncol = 2, scales = "free") +
  coord_flip() + theme_minimal() +
  theme(axis.text=element_text(size=14),
        axis.title=element_text(size=28,face="bold")) + 
  theme(strip.text.x = element_text(size = 18, colour = "black"))
  

```

Some of these words you might find surprising. The reviews can be searched to uncover the connections. 

You might want to search for dancing alligators & pink elephants are for instance.

All the best & bitte ein bit ;-)

```{r}
search_words <- c(" alligators ")

names(input_text) <- c("beer_type", "texts")
reviews <- input_text %>% 
  filter(str_detect(texts, search_words)) %>% 
  select(-beer_type) %>% 
  slice(1:1)

datatable(reviews, options = list(dom  = 't'))
```