---
title: Political ads on Facebook for the 2019 Belgian elections
author: William Bourgeois
date: '2020-01-04'
slug: political-ads-on-facebook-for-the-2019-belgian-elections
categories: []
tags:
  - Facebook
  - Belgian politics
  - nlp
  - topic modeling
---
I was looking for a way to have the same level of analysis of Facebook that is possible with the Twitter API, but gave up after a short while. The API seems to me quite closed in terms of what information is made available by Facebook. 

So I was quiet interested when I come across [this](http://rpubs.com/zoowalk/FB_EP2019) blog post of Mr Schmidt not on an API for Facebook generally, but for Facebook ads. 

The increased transparency comes as a reaction against criticism on Facebook for running 'manipulative' political adds (lies) on it's servers. 

Let's have a go at it (and thank you Mr. Schmidt ) and see what we can come up with related to the Belgian elections of May 2019 or more widely regarding Belgian politics. 

I am far from being an expert in Facebook advertising, so this is a blog on learning more about what we can get out of the API even though some are quite critical about the information it yeelds. See for instance [Facebookâ€™s Ad Archive API is Inadequate](https://blog.mozilla.org/blog/2019/04/29/facebooks-ad-archive-api-is-inadequate/).

And amid new leaks reported [today](https://www.theguardian.com/uk-news/2020/jan/04/cambridge-analytica-data-leak-global-election-manipulation) that show the vast extent of manipulation trough Facebook it seems important to have a closer look at what can be learned from the data Facebook provides. 

```{r message=FALSE, warning=FALSE}
library("tidyverse")
library("lubridate")
library("scales")
library("lemon") 
library("DT")
library("extrafont")
library("httr")
library("rvest")
library("glue")
# you might need the dev version of tidyr needed for unnest_wider function 
# (and R ver 3.6)
library("tidyr") # devtools::install_github("tidyverse/tidyr")
library("stringi")
library("tokenizers") 
library("reactable") #install.packages("reactable")
library("scales")
options(scipen = 999)
```

The way to go about is to go to Facebook for developers by creating an account and then create a new app. Add all the search fields you are interested in (I took all) and you then need a token that seems to be a temporary token (on this [portal](https://developers.facebook.com/tools/explorer/)), meaning that it is only valid for a certain time and you need to re-generate it for your next run. 

```{r eval=FALSE, include=TRUE}
# https://www.facebook.com/ads/library/api/?source=archive-landing-page
# link to fb api
my_link <- "https://graph.facebook.com"
my_token <- "put your token here"

#define fields you are interested in
search_fields=c("ad_creation_time", 
                "ad_delivery_start_time", 
                "ad_delivery_stop_time",
                "ad_creative_body",
                "ad_creative_link_caption",
                "ad_creative_link_description", 
                "ad_creative_link_title", 
                "ad_snapshot_url", 
                "page_id",
                "page_name",
                "currency",
                "spend",
                "demographic_distribution",
                "funding_entity",
                "impressions",
                "region_distribution") %>% 
  stringr::str_c(., collapse=", ")

page_one_response <- GET(my_link,
                         path = "/ads_archive",
                         query = list(access_token = my_token,
                                      limit=100,
                                      ad_active_status="ALL",
                                      search_terms="''",
                                      fields=search_fields,
                                      ad_reached_countries="BE"))
page_one_content<- content(page_one_response)

result <- tibble(data=page_one_content$data)

df_imp <- result %>% 
  tidyr::unnest_wider(data) 

#get the link refering to the next page
next_link <- page_one_content$paging$`next`

#iterate over all pages until there is no further page
while(length(next_link)>0) {
  next_response <- GET(next_link)
  next_content<- content(next_response)
  y <- tibble(data=next_content$data)
  df_next <- y %>%
    unnest_wider(data) 
  df_imp <- bind_rows(df_imp, df_next)  
  next_link <- next_content$paging$`next`
}

```

This returned 18K rows when I ran it 4 months ago. We are more than doubling the rows today, which is good news. It could have been that the archive only returned results on a rolling time window basis, but that does not seem to be the case, so more data to us. 

```{r include=FALSE}
df_imp <- readRDS("./data/20200104/df_imp_20200104.rds")
```

We received 16 variables. Some of them are lists. 

```{r}
names(df_imp)
```


It will be easier if we have a custom function that extracts the date from the different time stamps.

```{r}
make_date <- function(x){
  ymd(str_sub(x,1,10))
}

df_imp <- df_imp %>%
  mutate(ad_creation_day = make_date(ad_creation_time),
         ad_delivery_start_day = make_date(ad_delivery_start_time),
         ad_delivery_stop_day = make_date(ad_delivery_stop_time))

```



The archive starts at:
```{r}
min(df_imp$ad_delivery_start_day)
```

And ends today:

```{r}
max(df_imp$ad_delivery_start_day)
```
But I suspect the number of ads of the 'politics and issues' category we obtained are not evenly spread between these two dates.


```{r}
df_imp %>% 
  group_by(ad_delivery_start_day) %>% 
  summarise(n_ads=n()) %>%
  arrange(desc(n_ads)) %>%
  ggplot()+
  geom_bar(aes(x= ad_delivery_start_day,
               y=n_ads
               ),
           #color="transparent",
           stat="identity"
           )+
  theme_minimal()

```

So yes, very few until 2019, at least in the results we obtained with the api calls. This does not mean that there were not many in the past, it's just the data we got given to work with. 

The first dozens of ads were from the The European Democratic Party (EDP), the first ad from a Belgian political party started being delivered on 02/06/2017 and was from the N-VA:

```{r}
df_imp %>% 
  filter(ad_delivery_start_day == ymd("2017-06-02")) %>% 
  select(ad_delivery_start_day, ad_creative_body) %>%
  slice(1) %>% 
  datatable(class = 'compact', escape = TRUE, options = list(dom  = 't'))
```

Glad to hear the change worked.

Analyzing the global figures a bit further and getting the total ads per funding entity:

```{r}
df_imp %>% 
  count(funding_entity,
        sort = TRUE,
        name = "total ads")
```

We learn that most of the ads have no funding entity and that there are more UNICEF ads than ads for the party "Vlaams Belang". 

It is indeed a pity that Facebook decided to put the 'political' and 'issues' categories in one bag. Furthermore, these categories are not based on the type of message, ad, but on the type of funding entity or page. This means that different rules apply on 'issues' entities and business entities. 

In other words we have access to what Greenpeace Belgium is advertising in Belgium through Facebook, but not what Exxon Mobile is. 

[Because](https://www.facebook.com/ads/library/api/?source=archive-landing-page) as stated by Facebook: "Default value: "POLITICAL_AND_ISSUE_ADS": 
The type of ad. We label either all returned ads as political or issue ads, or label ads for news related to politics or issues of political importance. See Facebook Ads Help Center, About ads related to politics or issues of national importance. We currently only support POLITICAL_AND_ISSUE_ADS." Note that the options listed are: ALL, POLITICAL_AND_ISSUE_ADS, HOUSING_ADS, UNCATEGORIZED_ADS. 

(I'm not really holding my breath for the housing adds.)

Facebook's ads archive library is not only flawed, it is also biased because it does not include any corporate advertising. We might have a glimpse at what Belgian political parties spent and whatever their messages were, but we do not know how much was spent in Belgium flogging the F35 by Northrop Grumman for instance. And some commentators say that is exactly one of the reasons why Facebook is reluctant to be transparent. The more it is forced to be transparent on political ads and their impact on democracy, the more they will be under pressure to reveal the agenda of their corporate clients. As nicely explained in [this Guardian postcast](https://open.spotify.com/episode/09YkfvCUGsegLYgzMYrtvi?si=LniDF1c3QGezA9cmUERyfQ)

For now we have to do with what is given to us. Also with the fact that more than half of these ads have no known sponsors even though Facebook itself is [claiming](https://www.facebook.com/business/help/208949576550051) "For general or national-level elections taking place in the European Union (such as the general election in Spain, the federal election in Belgium and the presidential election in Lithuania), political parties and candidates campaigning in these elections will be required to complete ad authorisations and place  "Paid for by" disclaimers on all of their ads until the end of the election period."

Maybe the "Paid for by" appeared on all of their ads until the election period. Or they did not but here are the ads by unknown funding entity and by date in 2019:

```{r}
df_imp %>% 
  filter(ad_creation_day > ymd("2018-12-31")) %>% 
  filter(is.na(funding_entity)) %>% 
  group_by(ad_delivery_start_day) %>% 
  summarise(n_ads=n()) %>%
  arrange(desc(n_ads)) %>%
  ggplot()+
  geom_bar(aes(x= ad_delivery_start_day,
               y=n_ads
               ),
           #color="transparent",
           stat="identity"
           )+
  theme_minimal()

```

So we can safely say some of these ads were run during the election period, and it might be interesting to have a closer look at the ads without known funding entity, by looking at the page name they are financing.

```{r}
df_imp %>% 
  filter(is.na(funding_entity)) %>% 
  group_by(page_name) %>% 
  summarise(count = n()) %>%
  select(page_name, count) %>% 
  arrange(desc(count))
```

So here in the top ten, except for Mr. Mertens, there are no political actors. Only 'issues' like the UN agency UNICEF, a news outlet and others. 
 
This means that if we want to create any meaningful analysis of political ads on Facebook for Belgium based on this data set there is not a small amount of data cleaning to be done. Tbh we are used to that, but not in a data set that would have have been specifically designed to be a basis for analysis. 

Let's try to only keep politicians and parties. But before we do we will have a look at the estimation of total expenses for Belgium in 2019, per page. We do not have exact numbers of amounts spent only a range per ad. Here are the ranges:


```{r}
sort(as.numeric(unique(unlist(df_imp$spend))))
```

These seem like amazing amounts, but I have not yet taken into account the currencies...

```{r}
df_imp %>% 
  count(currency,
        sort = TRUE,
        name = "total ads")
```

Hmm 95 ads spent in rubles. That's always interesting these days. 

```{r}
rub <- df_imp %>% 
  filter(currency == "RUB")
```

The set contains an infamous scam messages but also some anti-China and anti-US messages from the Russian International Affairs Council:

```{r}
rub %>% 
  select(page_name, ad_creative_body) %>% 
  slice(79:80) %>% 
  mutate(ad_creative_body = substr(ad_creative_body, 1, 750)) %>% 
  datatable(class = 'compact', escape = TRUE, options = list(dom  = 't'))
```

Russia critising a possible withdrawal of US troops from Afghanistan is such a 2019 thing to do.

And the set of ads paid in rubles also contains what looks to be ads against Bart De Wever (the president of the N-VA), or click-bait, by these pages:

```{r}
rub <- rub %>% 
   filter(str_detect(ad_creative_body, "Wever")) %>% 
   select(page_name, ad_creative_body, ad_creative_link_description)%>% 
   unique() %>%
   head(4) 

```
```{r}
datatable(rub, rownames = FALSE, class = 'compact', escape = TRUE, options = list(dom  = 't'))

```

Rather weird. But it might be more interesting to see what ads had the biggest budgets. We need to convert to a common currency to compare the amounts spent. Let us get today's exchange rates to the euro of the currencies in our data frame.

```{r eval=FALSE}
library(quantmod)
from <- unique(df_imp$currency)
to <- c("EUR")
rates <- getQuote(paste0(from, to, "=X")) %>% 
  rownames_to_column() %>% 
  mutate(currency = substr(rowname,1,3)) %>% 
  select(currency, Last) 
```

```{r include=FALSE}
rates <- readRDS("./data/20200104/rates.rds")
```

```{r}
head(rates)
```

In order to estimate the amount spent per ad we will take the value at the middle of the spent range. 

```{r}
df_spent <- unnest_wider(df_imp, spend)
df_spent <- df_spent %>%
  mutate(spent = as.numeric(lower_bound)+(as.numeric(upper_bound)-as.numeric(lower_bound)+1)/2)
df_spent <- df_spent %>% 
  left_join(rates) %>% 
  mutate(euro = spent*Last)

df_spent %>% group_by(page_name) %>% 
  summarise(sum = sum(euro)) %>% 
  arrange(desc(sum))

```

So the Vlaams Belang takes first and second place, bearing in mind that the European Parliament spreads its ads over a much wider range of regions (hundreds, and there are three in Belgium). 

And here is an estimation of what was spent in rubles on these De Wever ads:

```{r}
df_spent %>%
  filter(currency == "RUB") %>% 
  filter(str_detect(ad_creative_body, "Wever")) %>%
  select(euro) %>% 
  sum()
```

6Kâ‚¬ is not that much, but too much to be a mere prank. 

What was the reach of these ads/pages? Or rather we have a range of impressions per ad (the number of times the content was displayed to a target audience). 

```{r}
df_reach <- df_spent %>%
  rename(lower_spent = lower_bound) %>% 
  rename(upper_spent = upper_bound) %>% 
  unnest_wider(impressions) %>% 
   mutate(impres = as.numeric(lower_bound)
          +(as.numeric(upper_bound)
            -as.numeric(lower_bound)+1)/2)
```

```{r}
df_reach %>% filter(!is.na(impres)) %>% 
  group_by(page_name) %>% 
  summarise(sum = sum(impres)) %>% 
  arrange(desc(sum)) 
```

This means that the ads of the N-VA and sp.a have had a bigger number of impressions than the page of Tom Van Grieken, despite having spent less money on them. Having said that reach and clicks are also important metrics to evaluate marketing impact, but we just do not have that data. 

Next to 'issue' pages there are also businesses in the list, like for instance Garnier. They cannot really be categorised as an issue organisation.

```{r}
df_reach %>% 
  filter(page_name == "Garnier") %>% 
  select(ad_creative_body) %>% 
  filter(str_detect(ad_creative_body, "luchtvervuiling!|Pollution")) %>% 
  datatable(class = 'compact', escape = TRUE, options = list(dom  = 't'))
```

Unless of course you see selling their water solution for wiping pollution from your face as an environmental cause...

One strategy to get only the political actors is to filter on the pages or funding entities that have a political party name in them. But some politicians do not mention in their page name for which party they are a candidate for. So all the candidates names are also needed. 

Let's do some scraping from an official election site.

```{r eval=FALSE, include=TRUE}
url <- paste0("https://elections2019.belgium.be/en/candidates?title=&field_election",
              "_id_value=&field_party_name_value=&order=field_party_name&sort=asc&page=0")

candidates <- data.frame()
selector <- paste0("body > div > main > div > div > section > div > div.views",
                   "-element-container.form-group > div > div.view-content > div > table")

for(i in (0:138)){
  print(i)
  url <- glue("https://elections2019.belgium.be/en/candidates?field_election_id_value",
              "=&field_party_name_value=&title=&page={i}")
  table <- url %>% 
  read_html() %>% 
  html_node(selector) %>% 
  html_table()
  
  candidates <- rbind(candidates, table)

}

```

```{r include=FALSE}
candidates <- readRDS("./data/20200104/candidates.rds")
```

```{r}
candidates %>%
  select(Name) %>% 
  unique() %>%
  nrow()
  
```

So almost 7 thousand candidates in 

```{r}
candidates %>%
  select(Party) %>% 
  unique() %>%
  nrow()

```

60 parties. 

We can start with the candidates, but then we need to construct a key to join the Facebook data with the candidates table. We will only use the sorted letters (without accents) of their first and last names.

```{r}
str_sort <- function(x)
  sapply(lapply(strsplit(x, NULL), sort), paste, collapse="")

candidates <- candidates %>%
  mutate(clean_name = str_replace_all(Name,"[^[:alnum:][:blank:]?&/\\-]", "")) %>%
  mutate(clean_name = tolower(str_replace_all(clean_name,"-", ""))) %>% 
  mutate(clean_name = stri_trans_general(clean_name, id = "Latin-ASCII")) %>%
  mutate(clean_name = str_replace_all(clean_name," ",""))%>% 
  mutate(key = str_sort(clean_name)) %>% 
  select(-clean_name)

df_candidates <- df_reach %>%
  mutate(clean_name = str_replace_all(page_name,"[^[:alnum:][:blank:]?&/\\-]", "")) %>%
  mutate(clean_name = tolower(str_replace_all(clean_name,"-", ""))) %>% 
  mutate(clean_name = stri_trans_general(clean_name, id = "Latin-ASCII")) %>%
  mutate(clean_name = str_replace_all(clean_name," ",""))%>% 
  mutate(key = str_sort(clean_name)) %>% 
  select(-clean_name)

df_candidates <- df_candidates %>% 
  inner_join(candidates)

df_candidates  %>% 
  count(page_name,
        sort = TRUE,
        name = "total ads")

```

And do some candidates names appear as funding entities for pages?

```{r}
df_candidates_f <- df_reach %>%
  filter(!page_name %in% df_candidates$page_name) %>% 
  mutate(clean_name = str_replace_all(funding_entity,"[^[:alnum:][:blank:]?&/\\-]", "")) %>%
  mutate(clean_name = tolower(str_replace_all(clean_name,"-", ""))) %>% 
  mutate(clean_name = stri_trans_general(clean_name, id = "Latin-ASCII")) %>%
  mutate(clean_name = str_replace_all(clean_name," ",""))%>% 
  mutate(key = str_sort(clean_name)) %>% 
  select(-clean_name)

df_candidates_f <- df_candidates_f %>% 
  inner_join(candidates)

df_candidates_f %>% 
  count(page_name, funding_entity,
        sort = TRUE,
        name = "total ads")

```

```{r}
df_candidates <- df_candidates %>% 
  rbind(df_candidates_f) %>% 
  filter(page_name !="Tekos")
```

Now using party names to identify page names and funding entities.

```{r}
parties <- candidates %>% 
  select(Party) %>% 
  mutate(name = tolower(Party)) %>% 
  select(-Party) %>% 
  unique()

parties <- c(parties,"one brussels") # adding

extract_party <- function(x){
  x <- unlist(tokenize_ngrams(x, lowercase = TRUE, n = 3L, n_min = 1,
                stopwords = character(), ngram_delim = " ", simplify = FALSE))
  first(parties$name[parties$name %in% x])
  }

df_parties <- df_reach %>%
  filter(!page_name %in% df_candidates$page_name)
  
df_parties$name_party <- map(tolower(df_parties$page_name), extract_party)

df_parties %>%
  count(unlist(name_party),
        sort = TRUE,
        name = "total ads")

# there was a problem treating the & and - in cd&v and n-va

additional_parties <- df_reach %>%
  filter(!page_name%in% df_candidates$page_name) %>% 
  mutate(name_party = tolower(str_extract(page_name, "N-VA|CD&V"))) %>% 
  filter(!is.na(name_party))

df_parties <- rbind(df_parties, additional_parties)

```

And for the party names in the funding entity names.

```{r}
df_parties_f <- df_reach %>%
  filter(!page_name %in% df_candidates$page_name)
  
df_parties_f$name_party <- map(tolower(df_parties_f$funding_entity), extract_party)

df_parties <- df_parties %>% 
  filter(!is.na(name_party)) %>% 
  rbind(df_parties_f %>% 
          filter((!is.na(name_party))))

# there was a problem treating the & and - in cd&v and n-va

additional_parties_f <- df_reach %>%
  filter(!page_name%in% df_candidates$page_name) %>% 
  mutate(name_party = tolower(str_extract(funding_entity, "N-VA|CD&V"))) %>% 
  filter(!is.na(name_party))

df_parties <- rbind(df_parties, additional_parties_f)

```

We still missed a few pages, but they do not represent a lot of ads. 

```{r message=FALSE, warning=FALSE}
more <- c("Jean-Claude Marcourt - Officiel", "Patrick Cottenie",
          "Guillaume Soupart - Conseiller communal", 
          "CÃ©dric Mahieu - Conseiller communal Ã  1030", 
          "Hannes De Reu", "Alexander Vandersmissen" ,
          "Marie Arena","Luc Haegemans", "Michael Freilich - Kamerlid", 
          "Christophe Leurident", 
          "Lara ThommÃ¨s - ConseillÃ¨re communale Ã  Ganshoren", 
          "Kevin Engelen","Marie Meunier", 
          "Thomas Van Hoof")


df_add_ref <- data.frame("page_name" = more, "name_party" = c("ps","nv-a","mr","cdh","sp.a",
                                                          "open vld","ps","cd&v",
                                                          "nv-a","mr","ps","nv-a",
                                                          "ps","n-va"))

df_add <- df_reach %>% 
  filter(page_name %in% more) %>% 
  left_join(df_add_ref)

df_parties <- df_parties %>% 
  rbind(df_add) %>% 
  unique()

```

There is a data frame based on the candidates, and another one based on the parties. Time to put them together. And we will be focusing on 2019.


```{r}

df_candidates <- df_candidates %>% 
  mutate(name_party = tolower(Party))

df <- df_parties %>% 
  rbind(df_candidates %>%
          select(-one_of(setdiff(names(df_candidates), names(df_parties))))) %>% 
  filter(page_name != "Mr. Gugu & Miss Go")

df <- df %>% 
  mutate(name_party = unlist(name_party)) %>% 
  filter(ad_creation_day > ymd("2018-12-31")) %>% 
  filter(ad_creation_day < ymd("2020-01-01"))

df %>% 
   count(name_party,
        sort = TRUE,
        name = "total_ads")

```


```{r include=FALSE}
# cleaning
rm(list=setdiff(ls(), "df"))
```

Here we define the parties colors and we count the number of ads per party:

```{r}
colors <- c("n-va"= "sienna3",
            "vlaams belang" = "grey35",
            "groen" = "lightgreen",
            "open vld" = "deepskyblue",
            "cdh" = "darkorange2",
            "ps" = "hotpink2",
            "sp.a" = "pink",
            "pvda" ="firebrick2",
            "ptb" = "firebrick2",
            "cd&v" = "orange",
            "ecolo" = "lightgreen",
            "dÃ©fi" = "#ca2c92",
            "mr" = "royalblue")


df %>% 
  group_by(name_party) %>% 
  summarise(n_ads=n()) %>%
  arrange(desc(n_ads)) %>%
  filter(n_ads > 200) %>% 
  ggplot() +
  geom_bar(aes(x=reorder(name_party, n_ads),
               y=n_ads,
               fill= name_party),
           color="transparent",
           stat="identity",
           width = 0.7) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(x = "", y = "#ads") +
  theme(axis.text.x = element_text(angle = 45, hjust = 0.95)) +
  scale_fill_manual("name_party", values=colors)
```

Doing the same but now looking at the amounts spent:

```{r message=FALSE, warning=FALSE}
euro_format <- function(largest_with_cents = 100000) {
  function(x) {
    x <- plyr::round_any(x, 0.01)
    if (max(x, na.rm = TRUE) < largest_with_cents &
        !all(x == floor(x), na.rm = TRUE)) {
      nsmall <- 2L
    } else {
      x <- plyr::round_any(x, 1)
      nsmall <- 0L
    }
    str_c("â‚¬", format(x, nsmall = nsmall, trim = TRUE, big.mark = ".", 
                      scientific = FALSE, digits=1L))
  }
}


df %>% 
  group_by(name_party) %>% 
  summarise(euro =sum(euro)) %>%
  arrange(desc(euro)) %>%
    filter(euro > 60000) %>% 
  ggplot() +
  geom_bar(aes(x=reorder(name_party, euro),
               y=euro,
               fill= name_party),
           color="transparent",
           stat="identity",
           width = 0.7) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(x = "", y = "") +
  #theme(axis.text.x = element_text(angle = 45, hjust = 0.70)) +
  scale_fill_manual("name_party", values=colors)+
  theme(plot.margin=unit(c(.75,.75,0,1,1),"cm"))+
  ylim(0, 2100000)+
  coord_flip()+
  scale_y_continuous(labels=euro_format())
#top, then right, bottom and left
```

The Vlaams Belang appears to be spending more than three times more than the second biggest spender, the N-VA. 

```{r}
(df %>% filter(name_party=="vlaams belang") %>% 
                 filter(year(ymd(ad_creation_day))==2019) %>% 
   select(euro) %>% 
   sum)/
sum(df$euro)
```

And their spending represents 42% of the total spending we identified in 2019.

What were the most spent on ads? 


```{r}
top_df <- df %>% 
  group_by(ad_creative_body, page_name, name_party) %>% 
  summarise(euro = sum(spent)) %>%
  arrange(desc(euro)) %>%
  ungroup() %>% 
  slice(1:5) %>% 
  datatable(class = 'compact', escape = TRUE, options = list(dom  = 't'))

# reactable(top_df, columns = list(
#   page_name = colDef(width = 90),
#   name_party = colDef(width = 90),
#   euro = colDef(width = 95 ,format = colFormat(prefix = "â‚¬", separators = TRUE, digits = 0))),
#           filterable = FALSE, minRows = 5)
```

This is the way the number of ads were spread out during the year by biggest advertisers:

```{r}
biggest <- df %>%
  filter(ad_delivery_start_day > ymd("2018-12-31")) %>% 
  group_by(name_party) %>% 
  summarise(n_ads=n()) %>%
  arrange(desc(n_ads)) %>% 
  slice(1:10)


df %>% 
  filter(ad_delivery_start_day > ymd("2018-12-31")) %>% 
  mutate(week = week(ad_creation_day)) %>% 
  filter(name_party %in% biggest$name_party) %>%
  #group_by(name_party, ad_delivery_start_day) %>%
  group_by(name_party, week) %>% 
  summarise(n_ads=n()) %>%
  arrange(desc(n_ads)) %>%
  ggplot()+
  theme_minimal() +
  labs(x = "party", y = "#ads") +
  geom_bar(aes(x= week,
               y=n_ads,
               fill = name_party),
           color="transparent",
           stat="identity",
           width = 0.7)+
  scale_fill_manual("party", values=colors)
```


And by amounts spent:

```{r message=FALSE, warning=FALSE}
df %>% 
  filter(ad_delivery_start_day > ymd("2018-12-31")) %>% 
  mutate(week = week(ad_creation_day)) %>% 
  filter(name_party %in% biggest$name_party) %>%
  #group_by(name_party, ad_delivery_start_day) %>%
  group_by(name_party, week) %>% 
  summarise(euro=sum(euro)) %>%
  arrange(desc(euro)) %>%
  ggplot()+
  theme_minimal() +
  labs(x = "party", y = "") +
  geom_bar(aes(x= week,
               y=euro,
               fill = name_party),
           color="transparent",
           stat="identity",
           width = 0.7)+
  scale_fill_manual("party", values=colors)+
  scale_y_continuous(labels=euro_format())

```

It also seems that the Vlaams Belang was the party that bought the most ads after the elections. Let's have a closer look:

```{r message=FALSE, warning=FALSE}
df %>% 
  filter(ad_creation_day > ymd("2019-05-31")) %>% 
  mutate(week = week(ad_creation_day)) %>% 
  filter(name_party %in% biggest$name_party) %>%
  #group_by(name_party, ad_delivery_start_day) %>%
  group_by(name_party, week) %>% 
  summarise(euro=sum(euro)) %>%
  arrange(desc(euro)) %>%
  ggplot()+
  theme_minimal() +
  labs(x = "party", y = "") +
  geom_bar(aes(x= week,
               y=euro,
               fill = name_party),
           color="transparent",
           stat="identity",
           width = 0.7)+
  scale_fill_manual("party", values=colors)+
  scale_y_continuous(labels=euro_format())

```

Only in week 40 was there some advertising from the PVDA and CD&V.

```{r}
df %>% 
  filter(ad_creation_day > ymd("2019-05-31")) %>% 
  mutate(week = week(ad_creation_day)) %>% 
  filter(week == 40) %>% 
  group_by(ad_creative_body, name_party) %>% 
  summarise(sum = sum(euro)) %>% 
  arrange(desc(sum)) %>% 
  slice(1:5) %>% 
  datatable(class = 'compact', escape = TRUE, options = list(dom  = 't'))
```

There are also some demo-graphical data at hand. And they need to be extracted from a column. Here under with a bit of a slow loop but I have to go and run errands anyway.

```{r eval=FALSE, include=TRUE}

df_demog <- data_frame()
for (i in (1:nrow(df))){
  print(i)
  x <- df[i,]
  y <- unnest(x, demographic_distribution)
  add_df <- data_frame()
  for (j in (1:nrow(y))){
    row <- data.frame(as.list(y$demographic_distribution[j]))
    add_df <- rbind(add_df, row)
  }
  y <- cbind(y, add_df)
  df_demog <- rbind(df_demog, y)
}


```

Let's what we have and what categories the parties target the most:

```{r}
#saveRDS(df_demog, "./data/20200104/df_demog.rds")
df_demog <- readRDS("./data/20200104/df_demog.rds")

df_demog <- df_demog %>% 
  mutate(percentage = as.numeric(as.character(percentage))) %>% 
  mutate(spent_demog = euro*percentage)

line_df <- df_demog %>% 
  select(name_party, age, gender, spent_demog) %>% 
  group_by(name_party, age, gender) %>% 
  summarise(spent = sum(spent_demog)) %>% 
  ungroup()


top_spenders <- line_df %>% group_by(name_party) %>% 
  summarise(budget = sum(spent)) %>% 
  arrange(desc(budget)) %>% 
  head(9) 

```

```{r message=FALSE}
line_df %>% left_join(top_spenders) %>% 
  mutate(prct = spent/budget) %>% 
  filter(name_party %in% top_spenders$name_party) %>%
  filter(age != "13-17") %>%
  filter(gender != "unknown") %>% 
  mutate(order  = as.numeric(substr(age,1,2))) %>% 
  arrange(order) %>% 
  ggplot(aes(x= reorder(age, order), y=prct, group=gender, color=gender)) +
  geom_line()+
  scale_color_manual(values=c("#CC6666", "#9999CC"))+
  facet_wrap(~name_party, scales = "fixed")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 90))+
  xlab("age bins")+
  ylab("percentage spent")+
  scale_y_continuous(labels=percent_format(accuracy=1))

```


The data can be further subsetted by one of the three Belgian regions but that is where it ends. Facebook is not transparent enough to present the categories that are used to micro target political messages. 

As Jason Kint put in a tweet:

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/kevin_purcell">@kevin_purcell</a> Please feel free to try (click the menu Embed Tweet here) and let me know if it does not work.</p>&mdash; Yihui Xie (@xieyihui) <a href="https://twitter.com/jason_kint/status/1213678664740753409">Jan 5, 2016</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

If acurate at all we can only infer some general trends from what we obtianed. 

In a future post I will be analysing the ad texts themselves. Let's see what we can learn from those!


