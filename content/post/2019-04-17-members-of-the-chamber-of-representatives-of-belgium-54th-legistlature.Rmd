---
title: members of the chamber of representatives of Belgium, 54th legistlature
author: William Bourgeois
date: '2019-04-17'
slug: members-of-the-chamber-of-representatives-of-belgium-54th-legistlature
categories: []
tags:
  - scraping
  - Belgian politics
  - magick
  - purrr
  - handeling files
---

For a country as small as Belgium 6 governments is a lot. It's maybe because we Belgians like to be governed and governed well. Why else would we want to pay for 6 governments, their administration and their parliaments? 

We also love politicians, so we want to have many. I also like politicians and decided to do some NLP on their tweets. But since there are a significant number of politicians in Belgium I searched for an objective criteria to define a subset. What better selection then the members of the national chamber of representatives? They were elected to represent us all at the national level and their tweets should somehow in some way be representative also.

https://www.dekamer.be is the official website of the chamber. Two lists are interesting here. One has the current members, the other one the complete list of members that at one moment or another were part of the parliament during the 54th legislature following the 2014 elections. 

Let's scrape them.
```{r message=FALSE}
library("tidyverse")
library("rvest")
library("XML")
library("glue")
library("magick")
```

```{r echo=TRUE}
url <- paste0("http://www.dekamer.be/kvvcr/showpage.cfm?section=/",
"depute&language=nl&cfm=/site/wwwcfm/depute/cvlist54.cfm")

table <- url %>% 
  read_html() %>% 
  html_nodes("#story > table") %>% 
  html_table()

table <- table[[1]] # extracting the dataframe

names(table) <- c("ln_fn", "party", "d1", "d2")
table <- table %>% 
  select(ln_fn, party) %>% 
  arrange(ln_fn)

head(table)

```

The white spaces between the last name and first some of some, like Wouter Beke need to be cleaned.

```{r}

table <- table %>%
  mutate(ln_fn = str_replace_all(ln_fn,"  +"," "))

nrow(table)
```

150 members today with their current political party. And the parties and number of seats are:

```{r}
table %>% 
  group_by(party) %>% 
  summarise(members=n()) %>% 
  arrange(desc(members))
```


We can also scrape the complete list of members of the 54th legislature. 

```{r}
url <- paste0("https://www.dekamer.be/kvvcr/showpage.cfm?section",
              "=/depute&language=nl&cfm=cvlist54.cfm?legis=54&today=n")

table_54 <- url %>% 
  read_html() %>% 
  html_nodes("#story > table") %>% 
  html_table()

table_54 <- table_54[[1]] # extracting the dataframe

names(table_54) <- c("ln_fn", "d1", "d2", "d3")  
table_54 <- table_54 %>% 
  select(ln_fn) %>% 
  arrange(ln_fn)

table_54 <- table_54 %>%
  mutate(ln_fn = str_replace_all(ln_fn,"  +"," "))


```

This table does not mention the party, which is a pity because some members have changed since 2014.

Who has left ?

```{r message=FALSE}

table_54 %>% 
  anti_join(table) %>% 
  unlist() %>% 
  unname()

```

So indeed 29 representatives. But some left to work in the government and then came back when their party decided to leave government because of a city in northern Africa. Go figure. 

But it does make the identification of a tweet as a tweet being sent by a member of parliament a little bit more complicated because we need to match the exact date of the tweet to the periods the politician was seating. I have a feeling this will imply some stupid hard coding. :weary:

Let's have some fun first. 

Looking at the page of the current members of parliament, and more specifically at the url leading to the members page, their identifier can be discovered. For instance Mrs Almaci has id 01189 for the website. So once the [xpath](https://www.w3schools.com/xml/xml_xpath.asp) is known it is relatively easy to extract the individual member's webpage url. 

```{r}
url <- paste0("http://www.dekamer.be/kvvcr/showpage.cfm?section",
              "=/depute&language=nl&cfm=cvlist54.cfm?legis=54&today=n")
page <- url %>% 
  read_html() %>% 
  html_nodes(xpath=  '//*[@id="story"]/table') 

# loop to get urls
urls <- tibble()

for(i in 1:nrow(table_54)){
  url <- xml_attrs(xml_child(xml_child(xml_child(page[[1]], i), 1), 1))
  url <- unname(url)
  name <- table_54$ln_fn[i]
  url <- cbind(name, url)
  urls <- rbind(urls, url)
}

# constructing the url
table_ids <- urls %>%
  mutate(url = paste0("http://www.dekamer.be/kvvcr/", url))   
  
table_ids %>%
  mutate(id = gsub('^.*key=*|\\s*&lact.*$','', url)) %>%
  mutate(row = rownames(.)) %>% 
  select(row, name, id) %>% 
  slice(29:39) %>% 
  head()

```

But there is a problem with matching the names and the ids.

```{r}
x <- table_ids
names <- tibble()

for (i in (1:nrow(x))){
  url <- x$url[i]
  fn_ln <- url %>%
    read_html() %>%
    html_nodes(xpath=  '//*[@id="myform"]/center/h2') %>% 
    html_text() %>% 
    as.data.frame()
  info <- cbind(fn_ln, url)
  names <- rbind(names, info)
}

names(names) <- c("fn_ln","url")

names <- names %>% 
  mutate(fn_ln =(str_replace_all(fn_ln,"  +"," ")))

names %>%
  mutate(id = gsub('^.*key=*|\\s*&lact.*$','', url)) %>%
  mutate(row = rownames(.)) %>% 
  select(row, fn_ln, id) %>% 
  slice(57:65) %>% 
  head()
  

```

The tables are sorted differently so there are wrong ids attributed to some parliamentarians (like Christophe D'Haese).

We need to correct that. Since the fn_ln are correct, we can build a key based on the letters in the name of the person.

```{r message=FALSE}

str_sort <- function(x)
  sapply(lapply(strsplit(x, NULL), sort), paste, collapse="")

w_key <- table_ids %>% 
  mutate(key = tolower(str_replace_all(name," ",""))) %>% 
  mutate(key = str_sort(key)) %>% 
  select(-url)

names_w_key <- names %>% 
  mutate(key = tolower(str_replace_all(fn_ln," ",""))) %>% 
  mutate(key = str_sort(key))

names <- names_w_key %>% 
  left_join(w_key) %>% 
  rename("ln_fn" = "name")

names %>% 
  select(-url) %>% 
  head()

```

Seems to work. Names are clean. Now the id code is needed. 

```{r}
# correct id
# extract their id from the url
names <- names %>%
  mutate(id = gsub('^.*key=*|\\s*&lact.*$','', url))   
```

And let's join the party where we can

```{r}
names <- names %>% 
  mutate(ln_fn = as.character(ln_fn)) %>% # transform from factor
  left_join(table)
```

Now we have in 'urls' the name and links to the personal webpage of the representatives on the chamber's website. Notice their id at the end of the url after 'cfm?key=' and before '&lat'. 

On their personal space there is more information like their parliament email, their personal website and a short cv. 

Let's only scrape their language for now.

```{r}

get_language <- function (url){
  as.character(url) %>% 
  read_html() %>% 
  html_node("td > p ") %>% 
  html_text() %>% 
  str_extract('(?<=Taal:\\s)\\w+') 
 }

names <- map_chr(names$url, get_language) %>% 
  cbind(names) 

names <- rename(names, language = .)

saveRDS(names, "./data/190417/names.rds")

head(names)
```

Let's get their mugshots. 

<center>
![Inspecting the code of the webpage](/img/20190417/inspect.png)
</center>

The images are in folder: http://www.dekamer.be/site/wwwroot/images/cv/


```{r}
# let's try purrr again

download_gifs <- function (name, id){
  url_pic <- glue("http://www.dekamer.be/site/wwwroot/images/cv/{id}.gif")
  download.file(url_pic, destfile = glue("./data/190417/{name}.gif"), 
                mode = "wb")
}

#download_gifs("test","01203")
```

That seems to work. Unfortunately while scraping it is not infrequent to encounter coding inconsistencies. For instance member "00902" does not have a "00902.gif" his is "902.gif". Dirty data alert :rotating_light:, shame on you webmaster :wink:. 


```{r eval=FALSE, include=FALSE}
#table_ids %>% 
#  map2(.x = name, .y = id, .f = download_gifs)

# 
# table_ids$id[table_ids$id == "00902"] <- 902
# table_ids$id[table_ids$id == "06918"] <- 6918
# table_ids$id[table_ids$id == "06536"] <- 6536
# table_ids$id[table_ids$id == "00951"] <- 951
# table_ids$id[table_ids$id == "06325"] <- 6325
# table_ids$id[table_ids$id == "06885"] <- 6885
# table_ids$id[table_ids$id == "06665"] <- 6665
# table_ids$id[table_ids$id == "01123"] <- 1123
# table_ids$id[table_ids$id == "06843"] <- 6843

map2(.x = table_ids$name, .y = table_ids$id, .f = download_gifs)

```

So sometimes the 0 is used and sometimes it is just the value without leading 0. One possible solution is to split the download_gifs function in two.

```{r}
download_gifs_with_0 <- function (name, id){
  url_pic <- glue("http://www.dekamer.be/site/wwwroot/images/cv/{id}.gif")
  download.file(url_pic, destfile = glue("./data/190417/down_1/{name}.gif"), 
                mode = "wb")
}

download_gifs_no_0 <- function (name, id){
  id <- as.numeric(id)
  url_pic <- glue("http://www.dekamer.be/site/wwwroot/images/cv/{id}.gif")
  download.file(url_pic, destfile = glue("./data/190417/down_2/{name}.gif"), 
                mode = "wb")
}

```

Trying to simplify the use of the purrr function 'safely', by Lionel Henry who I thank for his work & [presentation](https://www.youtube.com/watch?v=-v1tp41kizk) at the UseR2018 conference in Budapest.

```{r eval=FALSE, message=FALSE}
dir.create("./data/190417/down_1")
map2(.x = names$ln_fn, .y = names$id, safely(.f = download_gifs_with_0, otherwise = NULL))
```

So using 'safely' the error are being skipped and we managed to download how many files?

```{r echo=TRUE}
length(list.files("./data/190417/down_1"))
```

160. Only few missing. Let's use the other function.

```{r eval=FALSE}
dir.create("./data/190417/down_2")
map2(.x = names$ln_fn, .y = names$id, safely(.f = download_gifs_no_0, otherwise = NULL))
```

```{r echo=TRUE}
length(list.files("./data/190417/down_2"))
```

96. Hmm so now we have too many pictures (and the ones with no leading 0 seem older). See the aging of Mr Calvo: 
<center>
![Young Mr Calvo](/img/20190417/Calvo Kristof_young.gif) ![Old Mr Calvo](/img/20190417/Calvo Kristof_06128_old.gif)

</center>

Whose pics were additionally obtained?

```{r}
new <- list.files("./data/190417/down_1")
old <- list.files("./data/190417/down_2")
new_pics <- setdiff(old, new)
new_pics

```

Nice guy, Georges. Recently became grandfather again.

```{r message=FALSE, warning=FALSE, include=FALSE}
# copy the old and new to 'permanent' folder

lapply(new, function(x) file.copy(paste ("./data/190417/down_1", x , sep = "/"),  
                                        paste ("./data/190417",x, sep = "/"), recursive = FALSE,  copy.mode = TRUE))

lapply(new_pics, function(x) file.copy(paste ("./data/190417/down_2", x , sep = "/"),  
                                        paste ("./data/190417",x, sep = "/"), recursive = FALSE,  copy.mode = TRUE))


```
So are we missing some?

```{r}
fileslist <- list.files("./data/190417/", pattern = "*.gif")
names_list <- gsub(".gif","",fileslist)
setdiff(names_list, names$ln_fn)
```

We have a picture of all of them. :sunglasses:

A collage of their pictures will give us a nice overview. 

```{r message=FALSE, include=FALSE}
# code for a patch work
# https://masalmon.eu/2017/03/19/facesofr/

dir.create("./data/190417/patch_rep")
fileslist <- list.files("./data/190417/", pattern = "*.gif")
lapply(fileslist, function(x) file.copy(paste ("./data/190417", x , sep = "/"),
                                        paste("./data/190417/patch_rep",x, sep = "/"), 
                                        recursive = FALSE,  copy.mode = TRUE))
unlink("./data/190417/patch/Scourneau Vincent.gif") # his picture does not comply ;)

```

Need to rename them for the magick package.

```{r eval=FALSE}
setwd("./data/190417/patch_rep")
file.rename(list.files(), 
            paste0("g_", 1:179,".gif"))
```

Let's resize them for good measure. 

```{r, eval=FALSE}

resize_player <- function(x){
  img <- image_scale(image_read(x), "145x190!")
  image_write(img, x)        
}

player_pics <- list.files("./data/190417/patch_rep/", full.names = TRUE)

map(player_pics, resize_player)

```
Create columns
```{r eval=FALSE}
dir.create("./data/190417/cols_rep")

getwd()
files <- dir("data/190417/patch_rep", full.names = TRUE)
set.seed(1)
files <- sample(files, length(files)-2) # 176 will make a nice patch
gmp::factorize(length(files)-2)

no_rows <- 11
no_cols <- 16

make_column <- function(i, files, no_rows){
  image_read(files[(i*no_rows+1):((i+1)*no_rows)]) %>%
  image_append(stack = TRUE) %>%
    image_write(paste0("cols/", i, ".jpg"))
}

fun <- function(i, files, no_rows){
  image_read(files[(i*no_rows+1):((i+1)*no_rows)]) %>%
  image_append(stack = TRUE) %>%
    image_write(paste0("./data/190417/cols_rep/", i, ".jpg"))
}

 #could not really make map function ;-(

 for(i in (0:(no_cols-1))) {
 fun(i, files, no_rows)
 }
```
So we have the columns. Still need to bind them together.
```{r eval=FALSE}
getwd()

x <- image_read(dir(".data/190417/cols_rep/", full.names = TRUE))

img <- image_read(dir("data/190417/cols_rep/", full.names = TRUE)) %>%
image_append(stack = FALSE) 

setwd("./files/20190417")
getwd()
image_write(img,"2019-04-17-faces_of_54.jpg")

```

<center>
![reps 54](/img/20190417/2019-04-17-faces_of_54.jpg){width=800px height=800px} 
</center>


Looking at the picture I will let you judge how diverse the picture looks. Less diverse than our national soccer team, that is for sure. But those two groups are indeed two very small and different samples of Belgians. 

Wondering what the 55 legislature will look like. Will keep you posted.

But wait a minute. Did I say Belgian national soccer teams? Hold my beer.

Analyzing the Red Devil's website code, it appears the pictures are to be found in an aws bucket s3 folder. 

<center>
![Inspecting the code of the webpage](/img/20190417/Eden.png)
</center>

Downloading the pictures of the men:

```{r, eval=FALSE}

for(i in (1:7000)){
  
  url_pic <-glue("https://belgianfootball.s3.eu-central-1.amazonaws.com/",
                 "s3fs-public/rbfa/img/players/internationals/football/men/{i}.jpg")
  
  res <- try(download.file(url_pic, 
                           destfile = glue("./content/post/data/190417down_m/{i}.jpg"), 
                           mode = "wb"))
  if(inherits(res, "try-error"))
  {
    next
  }
  download.file(url_pic, destfile = glue("./content/post/data/190417down_m/{i}.jpg"), 
                mode = "wb")
  }


# clean the folder of empty files:

all_files <- dir("./content/post/data/190417down_m/", 
                 recursive = TRUE, full.names = TRUE)
erase <- all_files[file.info(all_files)[["size"]]< 4200]

## Remove empty files
unlink(erase, recursive=TRUE, force=FALSE)

```

Next is downloading the pictures of the women:

```{r, eval=FALSE}

for(i in (1:7000)){
  
  url_pic <-glue("https://belgianfootball.s3.eu-central-1.amazonaws.com/",
                 "s3fs-public/rbfa/img/players/internationals/football/women/{i}.jpg")
  
  res <- try(download.file(url_pic, destfile = glue("./content/post/data/190417down_w/{i}.jpg"), 
                           mode = "wb"))
  if(inherits(res, "try-error"))
  {
    next
  }
  download.file(url_pic, destfile = glue("./content/post/data/190417down_w/{i}.jpg"), 
                mode = "wb")
  }

# clean the folder of smaller files:

all_files <- dir("./content/post/data/190417down_w/", recursive = TRUE, full.names = TRUE)
erase <- all_files[file.info(all_files)[["size"]]< 4200]
## Remove empty files
unlink(erase, recursive=TRUE, force=FALSE)

```

There are now 687 pictures of national soccer players; 418 men and 269 women. Most of these pictures seem to be recent, but there is no date tag or something similar. Overall, they seem fairly recent.  

Sampling 176 of them:

```{r eval=FALSE}
# renaming files, so we do not have the same file names between men and women

setwd("./data/190417down_m/")
length(list.files())
file.rename(list.files(), paste0("m_", 1:418,".jpg"))
```
```{r eval=FALSE, include=FALSE}

setwd("./data/190417down_w/")
length(list.files())
file.rename(list.files(), 
            paste0("w_", 1:269,".jpg"))
```

Merging men and women.

```{r echo=TRUE}
men_files <- list.files("./data/190417down_m/", recursive = TRUE, full.names = TRUE)
women_files <- list.files("./data/190417down_w/", recursive = TRUE, full.names = TRUE)

player_files <- c(men_files, women_files)
set.seed(42)
sampled_files <- sample(player_files, 176)

```

Making the same patchwork

```{r eval=FALSE}

dir.create("./data/190417/patch")

lapply(sampled_files, function(x) file.copy(x,
                                        paste("./data/190417/patch",basename(x), 
                                              sep = "/"), 
                                        recursive = FALSE,  copy.mode = TRUE))


```

And the pictures need to have the same size (170x250 pixel)


```{r eval=FALSE}

resize_player <- function(x){
  img <- image_scale(image_read(x), "170x250!")
  image_write(img, x)        
}

player_pics <- list.files("./data/190417/patch/", full.names = TRUE)

map(player_pics, resize_player)

```

Create the columns.

```{r eval=FALSE}
dir.create("./data/190417/cols")

getwd()
files <- dir("data/190417/patch", full.names = TRUE)
set.seed(1)
files <- sample(files, length(files))
gmp::factorize(length(files))

no_rows <- 11
no_cols <- 16

make_column <- function(i, files, no_rows){
  image_read(files[(i*no_rows+1):((i+1)*no_rows)]) %>%
  image_append(stack = TRUE) %>%
    image_write(paste0("cols/", i, ".jpg"))
}
```



```{r eval=FALSE}
fun <- function(i, files, no_rows){
  image_read(files[(i*no_rows+1):((i+1)*no_rows)]) %>%
  image_append(stack = TRUE) %>%
    image_write(paste0("./data/190417/cols/", i, ".jpg"))
}

for(i in (0:(no_cols-1))) {
fun(i, files, no_rows)
}
```


```{r eval=FALSE}
img <- image_read(dir("data/190417/cols/", full.names = TRUE)) %>%
image_append(stack = FALSE) 

setwd("./files/20190417")
getwd()
image_write(img,"2019-04-17-faces_of_devils.jpg")
```



```{r eval=FALSE, include=FALSE}
unlink("./data/190417/cols",  recursive = TRUE)
unlink("./data/190417/patch",  recursive = TRUE)
```

So here is the patchwork of the national soccer players.

<center>
![devils](/img/20190417/2019-04-17-faces_of_devils.jpg)
</center>


So give me my tepid beer back. Here's a sample of our national soccer players. To contrast with our parliamentarian representatives. They are obviously younger, more diverse and fitter (let's hope). They also do not wear glasses (on the picture). And yes, I found an Easter egg. :-)


Let's get back to the parliamentarians tweets in the next post. 
